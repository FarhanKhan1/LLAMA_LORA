{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Documents/LLM/Llama-lora/chaingpt_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/ubuntu/Documents/LLM/Llama-lora/chaingpt_env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Documents/LLM/Llama-lora/chaingpt_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('@1=\\\\E[1~'), PosixPath('ct=\\\\E[3g'), PosixPath('ac=\\\\140\\\\140aaffggjjkkllmmnnooppqqrrssttuuvvwwxxyyzz{{||}}~~..--++,,hhII00'), PosixPath('AB=\\\\E[4%dm'), PosixPath('k0=\\\\E[10~'), PosixPath('as=\\\\E(0'), PosixPath('dc=\\\\E[P'), PosixPath('ae=\\\\E(B'), PosixPath('k4=\\\\EOS'), PosixPath('us=\\\\E[4m'), PosixPath('k7=\\\\E[18~'), PosixPath('cd=\\\\E[J'), PosixPath('cr=^M'), PosixPath('kN=\\\\E[6~'), PosixPath('ta=^I'), PosixPath('al=\\\\E[L'), PosixPath('up=\\\\EM'), PosixPath('pa#64'), PosixPath('AL=\\\\E[%dL'), PosixPath('kr=\\\\EOC'), PosixPath('k9=\\\\E[20~'), PosixPath('xv'), PosixPath('k6=\\\\E[17~'), PosixPath('xn'), PosixPath('RI=\\\\E[%dC'), PosixPath('dl=\\\\E[M'), PosixPath('k1=\\\\EOP'), PosixPath('im=\\\\E[4h'), PosixPath('sr=\\\\EM'), PosixPath('ti=\\\\E[?1049h'), PosixPath('DO=\\\\E[%dB'), PosixPath('le=^H'), PosixPath('km'), PosixPath('DC=\\\\E[%dP'), PosixPath('kH=\\\\E[4~'), PosixPath('kD=\\\\E[3~'), PosixPath('ks=\\\\E[?1h\\\\E='), PosixPath('bt=\\\\E[Z'), PosixPath('ue=\\\\E[24m'), PosixPath('@7=\\\\E[4~'), PosixPath('st=\\\\EH'), PosixPath('mi'), PosixPath('mb=\\\\E[5m'), PosixPath('LE=\\\\E[%dD'), PosixPath('kP=\\\\E[5~'), PosixPath('sc=\\\\E7'), PosixPath('op=\\\\E[39;49m'), PosixPath('cm=\\\\E[%i%d;%dH'), PosixPath('ei=\\\\E[4l'), PosixPath('vb=\\\\Eg'), PosixPath('am'), PosixPath('it#8'), PosixPath('DL=\\\\E[%dM'), PosixPath('kB=\\\\E[Z'), PosixPath('ku=\\\\EOA'), PosixPath('kl=\\\\EOD'), PosixPath('pf=\\\\E[4i'), PosixPath('cs=\\\\E[%i%d;%dr'), PosixPath('k3=\\\\EOR'), PosixPath('te=\\\\E[?1049l'), PosixPath('k8=\\\\E[19~'), PosixPath('co#203'), PosixPath('mh=\\\\E[2m'), PosixPath('kh=\\\\E[1~'), PosixPath('so=\\\\E[3m'), PosixPath('k2=\\\\EOQ'), PosixPath('bs'), PosixPath('po=\\\\E[5i'), PosixPath('Co#8'), PosixPath('k;=\\\\E[21~'), PosixPath('vi=\\\\E[?25l'), PosixPath('cl=\\\\E[H\\\\E[J'), PosixPath('do=^J'), PosixPath('ce=\\\\E[K'), PosixPath('ke=\\\\E[?1l\\\\E>'), PosixPath('Km=\\\\E[<'), PosixPath('LP'), PosixPath('nd=\\\\E[C'), PosixPath('ho=\\\\E[H'), PosixPath('F2=\\\\E[24~'), PosixPath('kd=\\\\EOB'), PosixPath('md=\\\\E[1m'), PosixPath('G0'), PosixPath('vs=\\\\E[34l'), PosixPath('UP=\\\\E[%dA'), PosixPath('li#55'), PosixPath('is=\\\\E)0'), PosixPath('SC|screen.xterm-256color|VT 100/ANSI X3.64 virtual terminal'), PosixPath('nw=\\\\EE'), PosixPath('AF=\\\\E[3%dm'), PosixPath('ve=\\\\E[34h\\\\E[?25h'), PosixPath('AX'), PosixPath('F1=\\\\E[23~'), PosixPath('bl=^G'), PosixPath('rc=\\\\E8'), PosixPath('IC=\\\\E[%d@'), PosixPath('rs=\\\\Ec'), PosixPath('pt'), PosixPath('kI=\\\\E[2~'), PosixPath('me=\\\\E[m'), PosixPath('ms'), PosixPath('se=\\\\E[23m'), PosixPath('mr=\\\\E[7m'), PosixPath('k5=\\\\E[15~')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/Documents/LLM/Llama-lora/chaingpt_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/ubuntu/Documents/LLM/Llama-lora/chaingpt_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pylab import rcParams\n",
    "import json\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"decapoda-research/llama-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=torch.float64 with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:13<00:00,  2.49it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float64,\n",
    "    device_map=\"auto\", \n",
    "    cache_dir = \"./Llama_weights\"\n",
    "\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    cache_dir = \"./Llama_weights\"\n",
    ")\n",
    "\n",
    "tokenizer.pad_token_id=(0)\n",
    "tokenizer.padding_side=\"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/json/default-64a2dfcf18e5361f/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 352.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Instruction', 'input', 'output'],\n",
       "    num_rows: 80216\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"all_question_answer_dataset.json\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"Instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_len = 256\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt=generate_prompt(data_point)\n",
    "    tokenized_full_prompt=tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                           \r"
     ]
    }
   ],
   "source": [
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=5000,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "MICRO_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAIN_STEPS = 3\n",
    "OUTPUT_DIR = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=100,\n",
    "    max_steps=TRAIN_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=training_arguments,\n",
    "        data_collator=data_collator\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 05:41, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )).__get__(model, type(model))\n",
    "model = torch.compile(model)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
